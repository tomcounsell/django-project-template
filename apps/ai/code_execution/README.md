# Safe LLM Code Execution Module

**Version**: 0.1.0 (Proof of Concept)
**Status**: Development Only - NOT Production Ready

## Overview

This module provides infrastructure for safely executing Python code generated by Large Language Models (LLMs) in a Django application. It implements multiple layers of security to mitigate risks while enabling productive code execution.

### ⚠️ Critical Security Warning

**The RestrictedPythonSandbox implementation in this proof of concept is NOT SECURE for production use.**

Pure Python sandboxing can be bypassed by determined attackers. For production deployments, you **MUST** use OS-level isolation:
- **E2B** (recommended for < 1000 executions/day)
- **gVisor** containers (self-hosted production)
- **Firecracker** microVMs (large scale)

See `/docs/SAFE_LLM_CODE_EXECUTION_ARCHITECTURE.md` for complete security guidance.

## Quick Start

### Basic Usage

```python
from apps.ai.code_execution import CodeExecutor

# Create executor instance
executor = CodeExecutor(user_id=request.user.id)

# Execute code
result = executor.execute("""
import math
radius = 5
area = math.pi * radius ** 2
print(f"Circle area: {area}")
""")

# Check result
if result.success:
    print(result.stdout)  # "Circle area: 78.53981633974483"
else:
    print(f"Error: {result.error_message}")
    print(f"Violations: {result.validation_violations}")
```

### With Execution Context

```python
executor = CodeExecutor(user_id=123)

result = executor.execute(
    code="""
user_data = context['user']
print(f"Processing data for {user_data['name']}")
    """,
    context={
        "user": {"name": "Alice", "id": 123}
    }
)
```

### Custom Configuration

```python
from apps.ai.code_execution import CodeExecutor
from apps.ai.code_execution.sandboxes import SandboxConfig

config = SandboxConfig(
    timeout_seconds=5,          # Shorter timeout
    max_memory_mb=256,          # Lower memory limit
    max_output_bytes=500_000,   # Smaller output limit
    allowed_imports=(           # Custom allowed modules
        "math",
        "json",
        "datetime",
    ),
)

executor = CodeExecutor(
    user_id=request.user.id,
    sandbox_config=config,
    enable_ast_validation=True,
    redact_sensitive_output=True,
)

result = executor.execute(code)
```

## Architecture

### Components

```
code_execution/
├── __init__.py              # Public API exports
├── exceptions.py            # Exception hierarchy
├── services/
│   └── executor.py          # Main orchestration logic
├── sandboxes/
│   ├── base.py              # Sandbox interface
│   └── restricted_python.py # Proof-of-concept sandbox
└── validators/
    ├── syntax_validator.py  # Syntax checking
    ├── ast_validator.py     # Security analysis
    └── output_validator.py  # Output sanitization
```

### Execution Flow

```
1. User submits code
   ↓
2. Syntax Validation
   - Parse code with ast.parse()
   - Validate structure
   ↓
3. AST Security Analysis
   - Scan for forbidden imports
   - Detect dangerous patterns
   - Check complexity limits
   ↓
4. Sandbox Execution
   - Restricted namespace
   - Timeout enforcement
   - Output capture
   ↓
5. Output Validation
   - Check size limits
   - Scan for sensitive data
   - Redact if configured
   ↓
6. Return Result
```

### Security Layers

1. **Import Restrictions** - Block dangerous modules (os, subprocess, socket, etc.)
2. **Namespace Control** - Limited builtins, no file access
3. **AST Analysis** - Detect security violations before execution
4. **Resource Limits** - Timeout, memory, output size
5. **Output Validation** - Sanitize results, detect sensitive data
6. **Logging** - Comprehensive audit trail

## API Reference

### CodeExecutor

Main class for executing code safely.

#### Constructor

```python
CodeExecutor(
    user_id: Optional[int] = None,
    sandbox_type: Type[BaseSandbox] = RestrictedPythonSandbox,
    sandbox_config: Optional[SandboxConfig] = None,
    enable_syntax_validation: bool = True,
    enable_ast_validation: bool = True,
    enable_output_validation: bool = True,
    redact_sensitive_output: bool = True,
    log_executions: bool = True,
)
```

**Parameters:**
- `user_id`: User ID for logging and rate limiting
- `sandbox_type`: Sandbox class to use
- `sandbox_config`: Custom configuration (uses defaults if None)
- `enable_syntax_validation`: Validate syntax before execution
- `enable_ast_validation`: Perform security analysis
- `enable_output_validation`: Validate and sanitize output
- `redact_sensitive_output`: Automatically redact sensitive data
- `log_executions`: Log execution attempts

#### execute()

```python
def execute(
    code: str,
    context: Optional[Dict[str, Any]] = None,
) -> ExecutionResult:
```

**Parameters:**
- `code`: Python code to execute
- `context`: Additional data available as `context` variable in code

**Returns:** `ExecutionResult` with comprehensive execution information

### ExecutionResult

Result object containing execution outcome and metadata.

**Attributes:**
- `success` (bool): Whether execution succeeded
- `stdout` (str): Standard output
- `stderr` (str): Standard error
- `error_message` (Optional[str]): Error description if failed
- `error_type` (Optional[str]): Exception class name
- `execution_time_seconds` (Optional[float]): Sandbox execution time
- `total_time_seconds` (Optional[float]): Total time including validation
- `validation_violations` (List[Dict]): Pre-execution violations
- `output_violations` (List[Dict]): Post-execution violations
- `was_output_redacted` (bool): Whether output was sanitized

**Methods:**
- `to_dict()`: Convert to dictionary for JSON serialization

### SandboxConfig

Configuration for sandbox execution (immutable dataclass).

**Attributes:**
- `timeout_seconds` (float): Maximum execution time (default: 30)
- `max_memory_mb` (int): Memory limit in MB (default: 512)
- `max_output_bytes` (int): Maximum output size (default: 1MB)
- `enable_network` (bool): Allow network access (default: False)
- `allowed_imports` (tuple[str]): Modules that can be imported
- `execution_context` (dict): Additional data for code
- `user_id` (Optional[int]): User executing code

### Exceptions

All exceptions inherit from `CodeExecutionError`:

- `ValidationError`: Pre-execution validation failed
- `SandboxError`: Sandbox setup/teardown failed
- `TimeoutError`: Execution exceeded time limit
- `ResourceLimitError`: Resource limits exceeded
- `SecurityViolationError`: Attempted forbidden operation
- `OutputValidationError`: Output validation failed

All exceptions have `to_dict()` method for structured error responses.

## Validators

### SyntaxValidator

Validates Python syntax without executing code.

```python
from apps.ai.code_execution.validators import SyntaxValidator

validator = SyntaxValidator()
result = validator.validate("print('hello')")

if not result.is_valid:
    print(f"Line {result.line_number}: {result.error_message}")
```

### ASTValidator

Analyzes code structure for security violations.

```python
from apps.ai.code_execution.validators import ASTValidator

validator = ASTValidator()
violations = validator.validate("import os; os.system('ls')")

for v in violations:
    print(f"{v.severity}: {v.message} at line {v.line_number}")
```

**Detected Patterns:**
- Forbidden imports (os, subprocess, socket, etc.)
- Dangerous functions (eval, exec, compile)
- Dangerous attributes (__import__, __builtins__)
- Excessive complexity (too many operations)

### OutputValidator

Validates and sanitizes execution output.

```python
from apps.ai.code_execution.validators import OutputValidator

validator = OutputValidator(redact=True)
result = validator.validate("API key: sk-abc123...")

print(result.sanitized_output)  # "API key: [REDACTED]"
print(result.violations)        # [{"type": "sensitive_data", ...}]
```

**Detected Patterns:**
- Social Security Numbers
- Credit card numbers
- API keys (AWS, OpenAI, generic)
- Database connection strings
- Private keys
- Passwords/secrets
- Excessive output size

## Sandboxes

### RestrictedPythonSandbox (Proof of Concept)

**Security Level:** Low - Development Only

Pure Python sandbox with:
- Custom `__import__` blocking dangerous modules
- Restricted `__builtins__` dictionary
- Timeout enforcement (Unix only)
- Output capture

**NOT secure for production use.**

### Future Sandbox Implementations

Placeholder interfaces exist for:
- `E2BSandbox` - Firecracker microVMs (recommended for production)
- `GVisorSandbox` - Container isolation

To implement production sandboxes:

```python
from apps.ai.code_execution.sandboxes import BaseSandbox

class E2BSandbox(BaseSandbox):
    def execute(self, code: str, config: SandboxConfig) -> SandboxResult:
        # 1. Initialize E2B sandbox
        # 2. Copy code to sandbox
        # 3. Execute with timeout
        # 4. Capture output
        # 5. Clean up sandbox
        # 6. Return result
        pass
```

## Configuration Examples

### Maximum Security (Untrusted Code)

```python
from apps.ai.code_execution import CodeExecutor
from apps.ai.code_execution.sandboxes import SandboxConfig

executor = CodeExecutor(
    user_id=request.user.id,
    sandbox_config=SandboxConfig(
        timeout_seconds=5,           # Short timeout
        max_memory_mb=256,          # Low memory limit
        max_output_bytes=100_000,   # Small output
        enable_network=False,       # No network
        allowed_imports=(           # Minimal imports
            "math",
            "json",
        ),
    ),
    enable_syntax_validation=True,
    enable_ast_validation=True,
    enable_output_validation=True,
    redact_sensitive_output=True,
    log_executions=True,
)
```

### Minimal Security (Trusted Users)

```python
executor = CodeExecutor(
    user_id=admin_user.id,
    sandbox_config=SandboxConfig(
        timeout_seconds=60,
        max_memory_mb=1024,
        allowed_imports=(
            "pandas",
            "numpy",
            "math",
            "json",
            "datetime",
            "collections",
        ),
    ),
    enable_ast_validation=False,    # Skip AST checks
    enable_output_validation=False, # Skip output validation
    redact_sensitive_output=False,
)
```

## Testing

### Running Tests

```bash
# Run all code execution tests
DJANGO_SETTINGS_MODULE=settings pytest apps/ai/code_execution/tests/ -v

# Run specific test file
DJANGO_SETTINGS_MODULE=settings pytest apps/ai/code_execution/tests/test_executor.py -v

# Run with coverage
DJANGO_SETTINGS_MODULE=settings pytest apps/ai/code_execution/tests/ --cov=apps/ai/code_execution
```

### Security Test Suite

The module includes adversarial test cases that attempt to:
- Import blocked modules
- Access forbidden functions
- Escape sandbox restrictions
- Cause resource exhaustion
- Exfiltrate sensitive data

See `tests/test_security.py` for examples.

## Logging

All executions are logged with structured data:

```python
import logging

logger = logging.getLogger('apps.ai.code_execution')

# Execution attempts logged at INFO level
logger.info("Code execution requested", extra={
    "user_id": 123,
    "code_length": 256,
})

# Failures logged at WARNING level
logger.warning("Code execution failed", extra={
    "user_id": 123,
    "error_type": "ValidationError",
})

# Security violations logged at WARNING level
logger.warning("High-severity security violations detected", extra={
    "user_id": 123,
    "violations": [...],
})
```

## Error Handling

### Exception Handling

```python
from apps.ai.code_execution import CodeExecutor, ValidationError, TimeoutError

executor = CodeExecutor()

try:
    result = executor.execute(code)
    # Always check result.success even if no exception
    if not result.success:
        handle_failure(result)
except ValidationError as e:
    # Code validation failed before execution
    print(f"Invalid code: {e.message}")
    print(f"Details: {e.details}")
except TimeoutError as e:
    # Execution took too long
    print(f"Timeout after {e.details['timeout_seconds']}s")
except Exception as e:
    # Unexpected error
    logger.exception("Unexpected error during code execution")
```

### Result-Based Error Handling

```python
result = executor.execute(code)

if not result.success:
    if result.validation_violations:
        # Pre-execution validation failed
        for violation in result.validation_violations:
            print(f"{violation['severity']}: {violation['message']}")
    elif result.error_type == "TimeoutError":
        # Execution timeout
        print("Code took too long to execute")
    else:
        # Runtime error during execution
        print(f"Runtime error: {result.error_message}")
```

## Integration with LLMs

### Providing Feedback for Self-Correction

```python
def execute_with_retry(code: str, max_retries: int = 3) -> ExecutionResult:
    """Execute code, providing errors to LLM for correction."""
    executor = CodeExecutor()

    for attempt in range(max_retries):
        result = executor.execute(code)

        if result.success:
            return result

        # Prepare error context for LLM
        error_context = {
            "error_type": result.error_type,
            "error_message": result.error_message,
            "violations": result.validation_violations,
            "attempt": attempt + 1,
        }

        # Ask LLM to fix the code
        code = llm.fix_code(code, error_context)

    return result
```

### Streaming Results

```python
def execute_streaming(code: str):
    """Execute code and stream output (future enhancement)."""
    # Current implementation captures all output
    # Future: stream stdout/stderr in real-time
    pass
```

## Performance Considerations

### Execution Times

Typical execution times (RestrictedPythonSandbox):
- Validation: 1-10ms
- Simple code (`print('hello')`): 1-5ms
- Data processing (pandas): 10-100ms
- Total overhead: ~10-20ms

Production sandboxes (E2B, gVisor):
- Sandbox startup: 100-200ms
- Execution: Similar to above
- Total overhead: ~150-250ms

### Optimization Tips

1. **Reuse executor instances** (but not across users)
2. **Disable unnecessary validation** for trusted code
3. **Use async execution** for long-running tasks
4. **Batch similar operations** when possible
5. **Cache compiled code** (future enhancement)

## Migration Path to Production

### Phase 1: Development (Current)
- Use `RestrictedPythonSandbox`
- Enable all validation layers
- Log everything
- Test with adversarial inputs

### Phase 2: Staging
- Implement `E2BSandbox` or `GVisorSandbox`
- Run parallel executions (old vs new sandbox)
- Compare results and performance
- Tune resource limits

### Phase 3: Production
- Switch to OS-level sandbox
- Keep `RestrictedPythonSandbox` as fallback only
- Monitor execution metrics
- Implement rate limiting per user

### Phase 4: Scale
- Add async task queue (Celery)
- Implement sandbox pooling
- Add execution result caching
- Set up dedicated execution infrastructure

## Security Checklist

Before deploying ANY code execution to production:

- [ ] Using OS-level sandboxing (E2B/gVisor/Firecracker)
- [ ] All validation layers enabled
- [ ] Output validation and redaction enabled
- [ ] Resource limits configured appropriately
- [ ] Comprehensive logging in place
- [ ] Rate limiting per user implemented
- [ ] Network access disabled or strictly controlled
- [ ] Regular security audit scheduled
- [ ] Incident response plan documented
- [ ] User education/documentation provided

## Troubleshooting

### Common Issues

**Import errors for allowed modules:**
- Ensure module is in `allowed_imports` tuple
- Check module is installed in environment
- Verify module name is root module (not submodule)

**Timeouts on simple code:**
- Check if running on Windows (no SIGALRM support)
- Increase `timeout_seconds` if legitimately slow
- Profile code to find performance bottlenecks

**Output truncation:**
- Increase `max_output_bytes` in config
- Process data in smaller chunks
- Return summary instead of full data

**False positive security violations:**
- Review AST validator configuration
- Adjust `forbidden_functions` set
- Use custom validator with different rules

## Further Reading

- `/docs/SAFE_LLM_CODE_EXECUTION_ARCHITECTURE.md` - Complete architecture guide
- `apps/ai/code_execution/tests/` - Test examples and security tests
- Python AST documentation: https://docs.python.org/3/library/ast.html
- OWASP Secure Coding: https://owasp.org/www-project-secure-coding-practices-quick-reference-guide/

## Contributing

When adding features to this module:

1. Write tests FIRST (TDD approach)
2. Update documentation in parallel with code
3. Add security test cases for new functionality
4. Document any new security considerations
5. Update this README with usage examples

## License

See project LICENSE file.

---

**Remember:** This is a proof of concept. Do not use `RestrictedPythonSandbox` in production.
