"""
Safe LLM Code Execution Module

This module provides infrastructure for safely executing Python code generated by
Large Language Models (LLMs) in a Django application environment.

SECURITY WARNING:
    Executing untrusted code is inherently dangerous. This module implements
    multiple layers of defense, but NO PURE PYTHON SOLUTION IS 100% SECURE.
    Production deployments MUST use OS-level isolation (gVisor, Firecracker, E2B).

Architecture Overview:
    1. Code Validation - AST analysis, syntax checking, security scanning
    2. Sandbox Selection - Choose appropriate isolation mechanism
    3. Execution - Run code with resource limits and monitoring
    4. Result Validation - Sanitize output, check for sensitive data
    5. Logging - Comprehensive audit trail

Key Components:
    - services/executor.py: Main orchestration logic
    - sandboxes/: Different isolation implementations
    - validators/: Pre and post-execution validation
    - models.py: Execution history and tracking

Usage Example:
    >>> from apps.ai.code_execution import CodeExecutor
    >>> executor = CodeExecutor(user_id=request.user.id)
    >>> result = executor.execute(code="print('Hello, World!')")
    >>> if result.success:
    ...     print(result.stdout)

See Also:
    - docs/SAFE_LLM_CODE_EXECUTION_ARCHITECTURE.md
    - apps/ai/code_execution/README.md
"""

from .exceptions import (
    CodeExecutionError,
    ResourceLimitError,
    SandboxError,
    SecurityViolationError,
    TimeoutError,
    ValidationError,
)
from .services.executor import CodeExecutor, ExecutionResult

__all__ = [
    "CodeExecutor",
    "ExecutionResult",
    "CodeExecutionError",
    "ValidationError",
    "SandboxError",
    "TimeoutError",
    "ResourceLimitError",
    "SecurityViolationError",
]

__version__ = "0.1.0"
